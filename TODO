Introduction to MPI, basic concepts

einarbeiten in valgrind -> an openmpi anpassen (openmpi neu kompilieren)

Use VecCreateGhost(MPI Comm comm,int n,int N,int nghost,int *ghosts,Vec *vv) instead of scatter routines

Peric -> PetscMapping über AO realisieren

nochmal .git manual durcharbeiten
http://rwehner.wordpress.com/2010/03/01/a-simple-way-to-create-git-repository-on-a-server-machine-connecting-via-ssh/

---------------------------------------------------------------------------

WICHTIG:

Nochmals komplette Order Verification durchspielen (mit Zeitabhängigkeit)
Comment Variables in index module
Solver mit nicht quadratischer domain testen (order verification)

automatisierung der gittergenerierung preprocessing und solve mittels bash-skript
Entweder DT in controlModule setzen oder aus processorFile lesen (dort dann aber auch Blending und URF setzen

MMS mit Trigonometrischen Funktionen für Skalarfeld und Geschwindigkeit

TAU

Einarbeitung Multigrid-Verfahren

---------------------------------------------------------------------------

OPTIONAL: 

FASTEST Tests recherchieren zur Skalierung beim Lösen von Transportgleichungen
Use Petsc LOG-FILES
Iterationen in GradFi einbauen
Use PETSC-Routine to determine Convergence: KSPDefaultConverged
preprocessing und grgen parallelisieren (müsste über geeignete input files machbar sein)
Mat get info verwenden

Delete PETSc Objects to avoid memory leak -> no memory leak

Remapping im preprocessing abschaffen!!! Am besten ist wohl ein prozessor locales remapping -> IJKPROC muss nur noch bei MIJK verwendet werden; L(F) sollte locale indices beinhalten, R(F) globale (Peric Indizierung) -> kann aber als inkohärent aufgefasst werden -> vorerst nicht implementiert

---------------------------------------------------------------------------

Aufbau eines procfiles

NUMBER OF BLOCKS
LOCAL BLOCK INDEXING
DT (0 -> STATIONARY)
BLENDING FACTOR (G)
UNDERRELAXATION FACTOR (URF)                                 
INFO LOCICAL

----------------------------------------------------------------------------------

GESRÄCH MIT PROFESSOR SCHÄFER

worum geht es in meiner Arbeit
	Titel der Arbeit
	Lösen einer allgemeinen Transportgleichung (instationär): Gleichung struktur : generische Transportgleichung
	Parallelisierung des Lösers mit PETSc (Beinhaltet Gleichungslöser und Framework)
wie untersuchen wir das
	Gittergenerierung: Blockweise Erzeugung des Gitters. 2 Randbedingungen: Dirichlet und Blockränder (innere Ränder)
	Preprocessing: Feststellen der Beziehungen zwischen den einzelnen Teilrechengebieten. Erzeugen eines Local-To-Global Mappings
	Solver: Lösen der PDGL mit einer FVM. Deferred Correction. Absenken des Residuums um 2 GO pro Äußere Iteration
wie ist Petsc aufgebaut
	Bild, mit Erklärung

was habe ich bisher gemacht (sequentiell - verifikation mittels mms (gleichung in Fortran, mupad) - multiblock seq - ver - parallel - ver)
wie verwende ich petsc: Mat,Vec,KSPSolve (mit output)
welche tests habe ich noch geplant: StrongScaling (optimierung, unterschiedliche Löser, PETSc Kofigurationen, Compiler), WeakScaling (optimierung), Ghost Values direkt in Vektoren speichern, Kommunikation von Gradienten über ein Field interlacing, verwenden von AO.

optional, falls noch Zeit ist: was möchte ich mit fastest vergleichen: inwiefern kann man multigrid mit KSP vergleichen? Rechenzeit, vor und nachteile beider Methoden gegenüberstellen und erklären

---------------------------------------------------------------------------------------

TESTS

STRONG SCALING:
    -fixed problem sizes (64^3, 128^3, 256^3 (512^3, 1024^3 falls interessant -> neue Manufactured solution ist anspruchsvoller))
    -erstmal nur auf einem Node, dann auch auf mehreren Nodes (einfluss des Netzwerkes)
    
WEAK SCALING (alpha=1):
    -variable problem sizes (46^3 auf einem Procs), (58^3 auf zwei Procs), (73^3 auf vier Procs), (92^3 auf 8 Procs), (116^3 auf 16 Procs).
    
    https://www.sharcnet.ca/help/index.php/Measuring_Parallel_Scaling_Performance
    
   


