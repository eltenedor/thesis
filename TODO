Introduction to MPI, basic concepts

einarbeiten in valgrind -> an openmpi anpassen (openmpi neu kompilieren)

Use VecCreateGhost(MPI Comm comm,int n,int N,int nghost,int *ghosts,Vec *vv) instead of scatter routines

Peric -> PetscMapping über AO realisieren

nochmal .git manual durcharbeiten
http://rwehner.wordpress.com/2010/03/01/a-simple-way-to-create-git-repository-on-a-server-machine-connecting-via-ssh/

---------------------------------------------------------------------------

WICHTIG:

Error Output in Textfile wieder implementieren
Nochmals komplette Order Verification durchspielen (mit Zeitabhängigkeit)
Comment Variables in index module
Mit Michel petsc configuration am FNB besprechen
Solver mit nicht quadratischer domain testen (order verification)

automatisierung der gittergenerierung preprocessing und solve mittels bash-skript
Variablengrößen an "maximal load" bei Kompilierung anpassen (Aufgabe von preprocessing.f90) -> preprocessing muss bereits über die prozessorbelastung informiert sein: Einlesen von proc_*.inp, sodass die locale Auslastung berechnet werden kann.

Entweder DT in controlModule setzen oder aus processorFile lesen (dort dann aber auch Blending und URF setzen

MMS mit Trigonometrischen Funktionen für Skalarfeld und Geschwindigkeit

TAU

---------------------------------------------------------------------------

OPTIONAL: 

Use Petsc LOG-FILES
Iterationen in GradFi einbauen
Use PETSC-Routine to determine Convergence: KSPDefaultConverged
preprocessing und grgen parallelisieren (müsste über geeignete input files machbar sein)
Mat get info verwenden

Delete PETSc Objects to avoid memory leak -> no memory leak

Remapping im preprocessing abschaffen!!! Am besten ist wohl ein prozessor locales remapping -> IJKPROC muss nur noch bei MIJK verwendet werden; L(F) sollte locale indices beinhalten, R(F) globale (Peric Indizierung) -> kann aber als inkohärent aufgefasst werden -> vorerst nicht implementiert

---------------------------------------------------------------------------

Aufbau eines procfiles

NUMBER OF BLOCKS
LOCAL BLOCK INDEXING
DT (0 -> STATIONARY)
BLENDING FACTOR (G)
UNDERRELAXATION FACTOR (URF)                                 
INFO LOCICAL

----------------------------------------------------------------------------------

GESRÄCH MIT PROFESSOR SCHÄFER

worum geht es in meiner Arbeit
wie untersuchen wir das
wie ist Petsc aufgebaut

was habe ich bisher gemacht (sequentiell - verifikation mittels mms - multiblock seq - ver - parallel - ver)
wie verwende ich petsc -> was wäre noch interessant zu testen
welche tests habe ich noch geplant

optional, falls noch Zeit ist: was möchte ich mit fastest vergleichen

---------------------------------------------------------------------------------------

TESTS

STRONG SCALING:
    -fixed problem sizes (64^3, 128^3, 256^3 (512^3, falls interessant))
    -erstmal nur auf einem Node, dann auch auf mehreren Nodes (einfluss des Netzwerkes)
    
WEAK SCALING (alpha=1):
    -variable problem sizes (46^3 auf einem Procs), (58^3 auf zwei Procs), (73^3 auf vier Procs), (92^3 auf 8 Procs), (116^3 auf 16 Procs).
    
    https://www.sharcnet.ca/help/index.php/Measuring_Parallel_Scaling_Performance
    
   


